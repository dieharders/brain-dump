{
  "mistral-7b": {
    "id": "mistral-7b",
    "name": "Mistral-7B",
    "archType": "transformer",
    "provider": "The Bloke",
    "licenses": ["Apache-2.0"],
    "description": "The Mistral-7B-v0.1 Large Language Model (LLM) is a pretrained generative text model with 7 billion parameters. Mistral-7B-v0.1 outperforms Llama 2 13B on all benchmarks we tested.",
    "fileSize": 4.37,
    "fileName": "mistral-7b-v0.1.Q4_K_M.gguf",
    "modelType": "mistral",
    "modelUrl": "https://huggingface.co/TheBloke/Mistral-7B-v0.1-GGUF",
    "quantTypes": ["Q5_K_S", "Q4_K_M", "Q3_K_S"],
    "context_window": 4096,
    "embedding_size": 32768,
    "num_gpu_layers": 32,
    "torch_dtype": "bfloat16",
    "downloadUrl": "https://huggingface.co/TheBloke/Mistral-7B-v0.1-GGUF/resolve/main/mistral-7b-v0.1.Q4_K_M.gguf",
    "sha256": "ce6253d2e91adea0c35924b38411b0434fa18fcb90c52980ce68187dbcbbe40c",
    "promptFormat": "{prompt}"
  },
  "mistral-7b-instruct-v0.2": {
    "id": "mistral-7b-instruct-v0.2",
    "name": "Mistral-7B-Instruct (uncensored)",
    "archType": "transformer",
    "provider": "The Bloke",
    "licenses": ["Apache-2.0"],
    "description": "The Mistral-7B-Instruct-v0.2 Large Language Model (LLM) is an improved instruct fine-tuned version of Mistral-7B-Instruct-v0.1.",
    "fileSize": 4.37,
    "fileName": "mistral-7b-instruct-v0.2.Q4_K_M.gguf",
    "modelType": "mistral",
    "modelUrl": "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF",
    "context_window": 4096,
    "embedding_size": 32768,
    "num_gpu_layers": 32,
    "torch_dtype": "bfloat16",
    "downloadUrl": "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf",
    "sha256": "3e0039fd0273fcbebb49228943b17831aadd55cbcbf56f0af00499be2040ccf9",
    "promptFormat": "<s>[INST]{prompt}[/INST]\n{response}</s>"
  },
  "llama-2-13b-chat": {
    "id": "llama-2-13b-chat",
    "name": "Llama 2 13B Chat",
    "archType": "transformer",
    "provider": "The Bloke",
    "licenses": ["llama2"],
    "description": "Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama-2-Chat, are optimized for dialogue use cases. Llama-2-Chat models outperform open-source chat models on most benchmarks we tested, and in our human evaluations for helpfulness and safety, are on par with some popular closed-source models like ChatGPT and PaLM",
    "fileSize": 7.87,
    "fileName": "llama-2-13b-chat.Q4_K_M.gguf",
    "modelType": "llama",
    "modelUrl": "https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF",
    "quantTypes": ["Q5_K_S", "Q4_K_M", "Q3_K_S"],
    "context_window": 4000,
    "embedding_size": 4096,
    "num_gpu_layers": 40,
    "torch_dtype": "float16",
    "downloadUrl": "https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF/resolve/main/llama-2-13b-chat.Q4_K_M.gguf",
    "sha256": "7ddfe27f61bf994542c22aca213c46ecbd8a624cca74abff02a7b5a8c18f787f",
    "promptFormat": "[INST]<<SYS>>\n{system_message}\n<</SYS>>\n{prompt}[/INST]"
  },
  "dolphin-2.6-mistral-7b.Q5_K_M": {
    "id": "dolphin-2.6-mistral-7b.Q5_K_M",
    "name": "Dolphin-Mistral-7B (uncensored)",
    "archType": "transformer",
    "provider": "The Bloke",
    "licenses": ["Apache-2.0"],
    "description": "This model is based on Mistral-7b. The base model has 16k context. This Dolphin is really good at coding, I trained with a lot of coding data. It is very obedient but it is not DPO tuned - so you still might need to encourage it in the system prompt as I show in the below examples.",
    "fileSize": 4.37,
    "fileName": "dolphin-2.6-mistral-7b.Q5_K_M.gguf",
    "modelType": "mistral",
    "modelUrl": "https://huggingface.co/TheBloke/dolphin-2.6-mistral-7B-GGUF",
    "quantTypes": ["Q5_K_S", "Q4_K_M", "Q3_K_S"],
    "context_window": 4096,
    "embedding_size": 32768,
    "num_gpu_layers": 32,
    "torch_dtype": "bfloat16",
    "downloadUrl": "https://huggingface.co/TheBloke/dolphin-2.6-mistral-7B-GGUF/resolve/main/dolphin-2.6-mistral-7b.Q5_K_M.gguf",
    "sha256": "e4ce9eabae27e45131c3d0d99223f133b96257301670073b3aee50f7627e20b2",
    "promptFormat": "<|im_start|>system\n{system_message}<|im_end|>\n<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant"
  },
  "mixtral-8x7b-v0.1.Q4_K_M": {
    "id": "mixtral-8x7b-v0.1.Q4_K_M",
    "name": "Mixtral-8x7B (uncensored)",
    "archType": "transformer",
    "provider": "The Bloke",
    "licenses": ["Apache-2.0"],
    "description": "The Mixtral-8x7B Large Language Model (LLM) is a pretrained generative Sparse Mixture of Experts. The Mistral-8x7B outperforms Llama 2 70B on most benchmarks we tested.",
    "fileSize": 24.6,
    "fileName": "mixtral-8x7b-v0.1.Q4_K_M.gguf",
    "modelType": "mixtral",
    "modelUrl": "https://huggingface.co/TheBloke/Mixtral-8x7B-v0.1-GGUF",
    "quantTypes": ["Q5_K_S", "Q4_K_M", "Q3_K_S"],
    "context_window": 4096,
    "embedding_size": 32768,
    "num_gpu_layers": 32,
    "torch_dtype": "bfloat16",
    "downloadUrl": "https://huggingface.co/TheBloke/Mixtral-8x7B-v0.1-GGUF/resolve/main/mixtral-8x7b-v0.1.Q4_K_M.gguf",
    "sha256": "5e066c60d89d904db46c3bf577661040578a223a5a39ba3fd23ff091549767f0",
    "promptFormat": "{prompt}"
  },
  "wizard-coder-python-13b": {
    "id": "wizard-coder-python-13b",
    "name": "WizardCoder 13B Python",
    "archType": "transformer",
    "provider": "The Bloke",
    "licenses": ["llama2"],
    "description": "Wizardlm: Empowering large language models to follow complex instructions. A StarCoder fine-tuned model using Evol-Instruct method specifically for coding tasks. Use this for code generation, also good at logical reasoning skills.",
    "fileSize": 7.87,
    "ramSize": 8,
    "fileName": "wizardcoder-python-13b-v1.0.Q4_K_M.gguf",
    "modelType": "llama",
    "context_window": 5120,
    "embedding_size": 16384,
    "num_gpu_layers": 40,
    "torch_dtype": "float16",
    "modelUrl": "https://huggingface.co/TheBloke/WizardCoder-Python-13B-V1.0-GGUF",
    "downloadUrl": "https://huggingface.co/TheBloke/WizardCoder-Python-13B-V1.0-GGUF/resolve/main/wizardcoder-python-13b-v1.0.Q4_K_M.gguf",
    "sha256": "50ff7a6a33357a063e0d09b6d43d95dbf62dda5450138541478e524e06a4fe2a",
    "promptFormat": "{system_message}\n\n### Instruction:\n{prompt}\n\n### Response:"
  },
  "openbuddy-openllama-7b-v12": {
    "id": "openbuddy-openllama-7b-v12",
    "name": "OpenBuddy 7B",
    "archType": "transformer",
    "provider": "The Bloke",
    "licenses": ["Apache-2.0"],
    "description": "OpenBuddy.ai - Open Multilingual Chatbot for Everyone. OpenBuddy is a powerful chatbot model with a focus on conversational AI and seamless multilingual capabilities. Built on top of the Falcon model from Tii, and the LLaMA model from Facebook, OpenBuddy offers enhanced performance and capabilities to handle complex conversational tasks.",
    "fileSize": 4.11,
    "ramSize": 7,
    "fileName": "openbuddy-openllama-7b-v12-bf16.Q4_K_M.gguf",
    "modelType": "llama",
    "modelUrl": "https://huggingface.co/TheBloke/openbuddy-openllama-7B-v12-bf16-GGUF",
    "quantTypes": ["Q5_K_S", "Q4_K_M", "Q3_K_S"],
    "context_window": 4096,
    "embedding_size": 4096,
    "num_gpu_layers": 32,
    "torch_dtype": "bfloat16",
    "downloadUrl": "https://huggingface.co/TheBloke/openbuddy-openllama-7B-v12-bf16-GGUF/resolve/main/openbuddy-openllama-7b-v12-bf16.Q4_K_M.gguf",
    "sha256": "98d9e5b27fcd70acdeac0fcc4c862356a6106464ac96d88e59c0d3b56ff5c27a",
    "promptFormat": "You are a helpful, AI Assistant named Buddy. You are talking to a human User.\nAlways answer as helpfully and logically as possible.\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\nYou can speak fluently in many languages, for example: English, Chinese.\nYou have vast knowledge, cutoff: 2021-09.{system_prompt}\n\nUser: {prompt}\nAssistant:"
  },
  "wizard-vicuna-13b": {
    "id": "wizard-vicuna-13b",
    "name": "Wizard Vicuna 13B (uncensored)",
    "archType": "transformer",
    "provider": "The Bloke",
    "licenses": ["Other"],
    "description": "This is wizard-vicuna-13b trained with a subset of the dataset - responses that contained alignment / moralizing were removed. The intent is to train a WizardLM that doesn't have alignment built-in, so that alignment (of any sort) can be added separately with for example with a RLHF LoRA.",
    "fileSize": 10,
    "fileName": "Wizard-Vicuna-13B-Uncensored.Q4_K_M.gguf",
    "modelType": "llama",
    "modelUrl": "https://huggingface.co/TheBloke/Wizard-Vicuna-13B-Uncensored-GGUF",
    "quantTypes": ["Q5_K_S", "Q4_K_M", "Q3_K_S"],
    "context_window": 5120,
    "embedding_size": 2048,
    "num_gpu_layers": 40,
    "torch_dtype": "float32",
    "downloadUrl": "https://huggingface.co/TheBloke/Wizard-Vicuna-13B-Uncensored-GGUF/resolve/main/Wizard-Vicuna-13B-Uncensored.Q4_K_M.gguf",
    "sha256": "e5ca843fd4a8c0a898b036f5c664a1fac366fb278f20880f1a750f38a950db73",
    "promptFormat": "{system_prompt} USER: {prompt} ASSISTANT:"
  },
  "law-llm": {
    "id": "law-llm",
    "name": "Law LLM",
    "archType": "transformer",
    "provider": "The Bloke",
    "licenses": ["Other"],
    "description": "Adapting Large Language Models via Reading Comprehension. We explore continued pre-training on domain-specific corpora for large language models. While this approach enriches LLMs with domain knowledge, it significantly hurts their prompting ability for question answering. Inspired by human learning via reading comprehension, we propose a simple method to transform large-scale pre-training corpora into reading comprehension texts, consistently improving prompting performance across tasks in biomedicine, finance, and law domains.",
    "fileSize": 4.08,
    "ramSize": 6.58,
    "fileName": "law-llm.Q4_K_M.gguf",
    "modelType": "llama",
    "modelUrl": "https://huggingface.co/TheBloke/law-LLM-GGUF",
    "quantTypes": ["Q5_K_S", "Q4_K_M", "Q3_K_S"],
    "context_window": 4096,
    "embedding_size": 2048,
    "num_gpu_layers": 32,
    "torch_dtype": "float16",
    "downloadUrl": "https://huggingface.co/TheBloke/law-LLM-GGUF/resolve/main/law-llm.Q4_K_M.gguf",
    "sha256": "d6b7e8d72c5394c0387bd0bc1742fe38ab8da82f2821778ced088574a8553c2b",
    "promptFormat": "{prompt}"
  },
  "orca-mini-7b": {
    "id": "orca-mini-7b",
    "name": "Orca Mini 7B (uncensored)",
    "archType": "transformer",
    "provider": "The Bloke",
    "licenses": ["Other"],
    "description": "An Uncensored LLaMA-7b model trained on explain tuned datasets, created using Instructions and Input from WizardLM, Alpaca & Dolly-V2 datasets and applying Orca Research Paper dataset construction approaches.",
    "fileSize": 4.08,
    "ramSize": 16,
    "fileName": "orca_mini_v3_7b.Q4_K_M.gguf",
    "modelType": "llama",
    "context_window": 4096,
    "embedding_size": 4096,
    "num_gpu_layers": 32,
    "torch_dtype": "float16",
    "modelUrl": "https://huggingface.co/TheBloke/orca_mini_v3_7B-GGUF",
    "downloadUrl": "https://huggingface.co/TheBloke/orca_mini_v3_7B-GGUF/resolve/main/orca_mini_v3_7b.Q4_K_M.gguf",
    "sha256": "77ea8409d75f2d5fd125afc346d5059a2411e8996a4e998a3643c945330d7baf",
    "promptFormat": "### System:\n{system_prompt}\n\n### User:\n{prompt}\n\n### Assistant:"
  },
  "zephyr-7b-beta": {
    "id": "zephyr-7b-beta",
    "name": "Zephyr 7B Beta",
    "archType": "transformer",
    "provider": "The Bloke",
    "licenses": ["MIT"],
    "description": "Zephyr is a series of language models that are trained to act as helpful assistants. Zephyr-7B-beta is the second model in the series, and is an uncensored, fine-tuned version of mistralai/Mistral-7B-v0.1. This model is recommended for RAG modes.",
    "fileSize": 4.37,
    "ramSize": 6.87,
    "fileName": "zephyr-7b-beta.Q4_K_M.gguf",
    "modelType": "mistral",
    "context_window": 32768,
    "embedding_size": 4096,
    "num_gpu_layers": 32,
    "torch_dtype": "bfloat16",
    "modelUrl": "https://huggingface.co/TheBloke/zephyr-7B-beta-GGUF",
    "downloadUrl": "https://huggingface.co/TheBloke/zephyr-7B-beta-GGUF/resolve/main/zephyr-7b-beta.Q4_K_M.gguf",
    "sha256": "503580dce392c6e64669ad21a77023ba2a17baa0c381250fb67c11ba6406a85e",
    "promptFormat": "<|system|>\n{system_prompt}</s>\n<|user|>\n{prompt}</s>\n<|assistant|>\n"
  },
  "bling-sheared-llama-1.3b-0.1": {
    "id": "bling-sheared-llama-1.3b-0.1",
    "name": "Sheared Llama 1.3B",
    "archType": "transformer",
    "provider": "Aryanne",
    "licenses": ["Apache-2.0"],
    "description": "Instruct trained on top of a Sheared-LLaMA-1.3B base model. BLING is designed for enterprise automation use cases, especially in knowledge-intensive industries, such as financial services, legal and regulatory industries with complex information sources. BLING models try to focus on a narrower set of Instructions more suitable to a ~1B parameter GPT model. BLING is ideal for rapid prototyping, testing, and the ability to perform an end-to-end workflow locally on a laptop without having to send sensitive information over an Internet-based API. The first BLING models have been trained for common RAG scenarios, specifically: question-answering, key-value extraction, and basic summarization as the core instruction types without the need for a lot of complex instruction verbiage - provide a text passage context, ask questions, and get clear fact-based responses.",
    "fileSize": 2.69,
    "ramSize": 1,
    "fileName": "f16-bling-sheared-llama-1.3b-0.1.gguf",
    "modelType": "llama",
    "context_window": 2048,
    "embedding_size": 4096,
    "num_gpu_layers": 24,
    "torch_dtype": "float32",
    "modelUrl": "https://huggingface.co/llmware/bling-sheared-llama-1.3b-0.1",
    "downloadUrl": "https://huggingface.co/Aryanne/Bling-Sheared-Llama-1.3B-0.1-gguf/resolve/main/f16-bling-sheared-llama-1.3b-0.1.gguf",
    "sha256": "38e1a893f9a9949be324ad8edacf627f60c7626432b622a6bed155af6773bf6c",
    "promptFormat": "<human>: {prompt}\n<bot>:"
  },
  "example-cat-anim": {
    "id": "example-cat-anim",
    "name": "Example Cute Cat Animation",
    "provider": "giphy",
    "licenses": ["Academic", "Commercial", "Other"],
    "description": "This is a test file (gif) for testing download behavior.",
    "fileSize": 3060203,
    "context_window": 4000,
    "fileName": "cute-cat-anim.gif",
    "downloadUrl": "https://media.giphy.com/media/04uUJdw2DliDjsNOZV/giphy.gif"
  }
}
