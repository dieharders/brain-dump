{
  "TheBloke/Mistral-7B-v0.1-GGUF": {
    "repoId": "TheBloke/Mistral-7B-v0.1-GGUF",
    "name": "Mistral 7B (chat)",
    "description": "The Mistral-7B-v0.1 Large Language Model (LLM) is a pretrained generative text model with 7 billion parameters. Mistral-7B-v0.1 outperforms Llama 2 13B on all benchmarks we tested.",
    "context_window": 4096,
    "num_gpu_layers": 32,
    "messageFormat": "{system_message}\n\n{prompt}"
  },
  "TheBloke/Mistral-7B-Instruct-v0.2-GGUF": {
    "repoId": "TheBloke/Mistral-7B-Instruct-v0.2-GGUF",
    "name": "Mistral 7B v0.2 (instruct)",
    "description": "The Mistral-7B-Instruct-v0.2 Large Language Model (LLM) is an improved instruct fine-tuned version of Mistral-7B-Instruct-v0.1.",
    "context_window": 4096,
    "num_gpu_layers": 32,
    "messageFormat": "<s>[INST] {system_message}\n\n{prompt} [/INST]"
  },
  "TheBloke/Llama-2-13B-chat-GGUF": {
    "repoId": "TheBloke/Llama-2-13B-chat-GGUF",
    "name": "Llama 2 13B (chat)",
    "description": "Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama-2-Chat, are optimized for dialogue use cases. Llama-2-Chat models outperform open-source chat models on most benchmarks we tested, and in our human evaluations for helpfulness and safety, are on par with some popular closed-source models like ChatGPT and PaLM",
    "context_window": 4096,
    "num_gpu_layers": 40,
    "messageFormat": "[INST]<<SYS>>\n{system_message}\n<</SYS>>\n{prompt}[/INST]"
  },
  "TheBloke/llama2_7b_chat_uncensored-GGUF": {
    "repoId": "TheBloke/llama2_7b_chat_uncensored-GGUF",
    "name": "Llama 2 7B (chat) (uncensored)",
    "description": "Uncensored Llama 2 model by George Sung and Jarrad Hope. Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama-2-Chat, are optimized for dialogue use cases. Llama-2-Chat models outperform open-source chat models on most benchmarks we tested, and in our human evaluations for helpfulness and safety, are on par with some popular closed-source models like ChatGPT and PaLM",
    "context_window": 2048,
    "num_gpu_layers": 32,
    "messageFormat": "### HUMAN:\n{system_message}\n\n{prompt}\n\n### RESPONSE:"
  },
  "TheBloke/Llama-2-7B-32K-Instruct-GGUF": {
    "repoId": "TheBloke/Llama-2-7B-32K-Instruct-GGUF",
    "name": "Llama 2 7B 32k (instruct) (uncensored)",
    "description": "Llama-2-7B-32K-Instruct is an open-source, long-context chat model finetuned from Llama-2-7B-32K, over high-quality instruction and chat data. Llama-2-7B-32K-Instruct is fine-tuned over a combination of two parts: 1. 19K single- and multi-round conversations generated by human instructions and Llama-2-70B-Chat outputs. 2. Long-context Summarization and Long-context QA.",
    "context_window": 32000,
    "num_gpu_layers": 32,
    "messageFormat": "[INST]\n{system_message}\n\n{prompt}\n[/INST]"
  },
  "TheBloke/CodeLlama-7B-GGUF": {
    "repoId": "TheBloke/CodeLlama-7B-GGUF",
    "name": "Code Llama 7B (coding)",
    "description": "Code Llama is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 34 billion parameters. This is the repository for the base 7B version in the Hugging Face Transformers format. This model is designed for general code synthesis and understanding. Links to other models can be found in the index at the bottom.",
    "context_window": 16384,
    "num_gpu_layers": 32,
    "messageFormat": "{prompt}"
  },
  "TheBloke/dolphin-2.6-mistral-7B-GGUF": {
    "repoId": "TheBloke/dolphin-2.6-mistral-7B-GGUF",
    "name": "Dolphin Mistral 7B (uncensored)",
    "description": "This model is based on Mistral-7b. The base model has 16k context. This Dolphin is really good at coding, I trained with a lot of coding data. It is very obedient but it is not DPO tuned - so you still might need to encourage it in the system prompt as I show in the below examples.",
    "context_window": 32768,
    "num_gpu_layers": 32,
    "messageFormat": "<|im_start|>system\n{system_message}<|im_end|>\n<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant"
  },
  "TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF": {
    "repoId": "TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF",
    "name": "Mixtral 8x7B Instruct (scientific)",
    "description": "The Mixtral-8x7B Large Language Model (LLM) is a pretrained generative Sparse Mixture of Experts. The Mistral-8x7B outperforms Llama 2 70B on most benchmarks we tested.",
    "context_window": 32768,
    "num_gpu_layers": 32,
    "messageFormat": "[INST] {system_message} {prompt} [/INST]"
  },
  "TheBloke/WizardCoder-Python-13B-V1.0-GGUF": {
    "repoId": "TheBloke/WizardCoder-Python-13B-V1.0-GGUF",
    "name": "WizardCoder 13B Python",
    "description": "Wizardlm: Empowering large language models to follow complex instructions. A StarCoder fine-tuned model using Evol-Instruct method specifically for coding tasks. Use this for code generation, also good at logical reasoning skills.",
    "context_window": 5120,
    "num_gpu_layers": 40,
    "messageFormat": "{system_message}\n\n### Instruction:\n{prompt}\n\n### Response:"
  },
  "TheBloke/Luna-AI-Llama2-Uncensored-GGUF": {
    "repoId": "TheBloke/Luna-AI-Llama2-Uncensored-GGUF",
    "name": "Luna AI Llama2 (uncensored)",
    "description": "Wizardlm: Empowering large language models to follow complex instructions. A StarCoder fine-tuned model using Evol-Instruct method specifically for coding tasks. Use this for code generation, also good at logical reasoning skills.",
    "messageFormat": "USER: {system_message}\n\n{prompt}\nASSISTANT:"
  },
  "TheBloke/openbuddy-openllama-7B-v12-bf16-GGUF": {
    "repoId": "TheBloke/openbuddy-openllama-7B-v12-bf16-GGUF",
    "name": "OpenBuddy 7B",
    "description": "OpenBuddy.ai - Open Multilingual Chatbot for Everyone. OpenBuddy is a powerful chatbot model with a focus on conversational AI and seamless multilingual capabilities. Built on top of the Falcon model from Tii, and the LLaMA model from Facebook, OpenBuddy offers enhanced performance and capabilities to handle complex conversational tasks.",
    "context_window": 4096,
    "num_gpu_layers": 32,
    "messageFormat": "You can speak fluently in many languages, for example: English, Chinese.\nYour vast knowledge is cutoff: 2021-09.{system_message}\n\nUser: {prompt}\nAssistant:"
  },
  "TheBloke/Wizard-Vicuna-13B-Uncensored-GGUF": {
    "repoId": "TheBloke/Wizard-Vicuna-13B-Uncensored-GGUF",
    "name": "Wizard Vicuna 13B (uncensored)",
    "description": "This is wizard-vicuna-13b trained with a subset of the dataset - responses that contained alignment / moralizing were removed. The intent is to train a WizardLM that doesn't have alignment built-in, so that alignment (of any sort) can be added separately with for example with a RLHF LoRA.",
    "context_window": 5120,
    "num_gpu_layers": 40,
    "messageFormat": "{system_message} USER: {prompt} ASSISTANT:"
  },
  "TheBloke/law-LLM-GGUF": {
    "repoId": "TheBloke/law-LLM-GGUF",
    "name": "Law LLM (chat)",
    "description": "Adapting Large Language Models via Reading Comprehension. We explore continued pre-training on domain-specific corpora for large language models. While this approach enriches LLMs with domain knowledge, it significantly hurts their prompting ability for question answering. Inspired by human learning via reading comprehension, we propose a simple method to transform large-scale pre-training corpora into reading comprehension texts, consistently improving prompting performance across tasks in biomedicine, finance, and law domains.",
    "context_window": 4096,
    "num_gpu_layers": 32,
    "messageFormat": "{system_message}\n\n{prompt}"
  },
  "TheBloke/orca_mini_v3_7B-GGUF": {
    "repoId": "TheBloke/orca_mini_v3_7B-GGUF",
    "name": "Orca Mini 7B (instruct) (uncensored)",
    "description": "An Uncensored LLaMA-7b model trained on explain tuned datasets, created using Instructions and Input from WizardLM, Alpaca & Dolly-V2 datasets and applying Orca Research Paper dataset construction approaches.",
    "context_window": 4096,
    "num_gpu_layers": 32,
    "messageFormat": "### System:\n{system_message}\n\n### User:\n{prompt}\n\n### Assistant:"
  },
  "TheBloke/zephyr-7B-beta-GGUF": {
    "repoId": "TheBloke/zephyr-7B-beta-GGUF",
    "name": "Zephyr 7B Beta (search) (uncensored)",
    "description": "Zephyr is a series of language models that are trained to act as helpful assistants. Zephyr-7B-beta is the second model in the series, and is an uncensored, fine-tuned version of mistralai/Mistral-7B-v0.1. This model is recommended for RAG modes.",
    "context_window": 32768,
    "num_gpu_layers": 32,
    "messageFormat": "<|system|>\n{system_message}</s>\n<|user|>\n{prompt}</s>\n<|assistant|>\n"
  },
  "Aryanne/Bling-Sheared-Llama-1.3B-0.1-gguf": {
    "repoId": "Aryanne/Bling-Sheared-Llama-1.3B-0.1-gguf",
    "name": "Sheared Llama 1.3B (search)",
    "description": "Instruct trained on top of a Sheared-LLaMA-1.3B base model. BLING is designed for enterprise automation use cases, especially in knowledge-intensive industries, such as financial services, legal and regulatory industries with complex information sources. BLING models try to focus on a narrower set of instructions more suitable to a ~1B parameter GPT model. BLING is ideal for rapid prototyping, testing, and the ability to perform an end-to-end workflow locally on a laptop without having to send sensitive information over an Internet-based API. The first BLING models have been trained for common RAG scenarios, specifically: question-answering, key-value extraction, and basic summarization as the core instruction types without the need for a lot of complex instruction verbiage - provide a text passage context, ask questions, and get clear fact-based responses.",
    "context_window": 2048,
    "num_gpu_layers": 24,
    "messageFormat": "<human>: {context_message}\n{prompt}\n<bot>:"
  }
}
